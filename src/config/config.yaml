defaults:
  - task: ???
  - dataset: mednlp-de
#wandb args  
project_name: huggingface 
run_name: None
# Training Arguments
batch_size: 8
evaluation_strategy: "epoch"
save_strategy: "epoch"
learning_rate: 2e-5
num_train_epochs: 10
weight_decay: 0.01
load_best_model_at_end: True
metric_for_best_model: "f1"
save_total_limit: 1
fp16: False

# Model
pretrained_model: "deepset/gbert-base"
#pretrained_model: "cl-tohoku/bert-base-japanese-whole-word-masking"

# Inference
threshold: 0.5
