defaults:
  - task: ???
  - dataset: mednlp-de
#wandb args  
project_name: huggingface 
run_name: None
# Training Arguments
batch_size: 12 
evaluation_strategy: "epoch"
save_strategy: "epoch"
learning_rate: 3.321e-05
num_train_epochs: 15
weight_decay: 0.15
load_best_model_at_end: True
metric_for_best_model: "f1"
save_total_limit: 1
fp16: False

# Model
pretrained_model: "deepset/gbert-base"
#pretrained_model: "cl-tohoku/bert-base-japanese-whole-word-masking"

# Inference
threshold: 0.7
